This repository provides a machine learning model trained on the ACME Happiness Survey dataset from Apziva. The model’s goal is to predict customer satisfaction directly from survey responses.

# Prerequisites
To run the code, the following packages must be installed:
- pandas
- numpy
- matplotlib and seaborn
- scikit-learn
- seaborn


# LazyPredict :

is a powerful Python library designed to streamline and partially automate machine learning workflows. It rapidly generates a wide range of baseline models with minimal coding effort, enabling users to efficiently compare algorithm performance prior to any hyperparameter tuning



# Fitting and Evaluating the Models

The table generated by LazyPredict provides a quick comparison of multiple classification models based on key performance metrics such as Accuracy, Balanced Accuracy, ROC AUC, F1 Score, and computation time. These metrics help evaluate how well each model handles both balanced and imbalanced data, measures class‑separation ability, and balances precision with recall. Among all tested models, Based on the results of Lazy Predict, we choose the top 3 models, which are Extra Tree Classifier, XGB Classifier and Label Propagation Extra Tree Classifier, XGB Classifier delivered the strongest prediction results,

# Confusion matrix and classification report

Based on the results, we can see that the XGB Classifier Model is the best for predicting class 0 unhappy customers, as it has an f1 score of 0.67, which is higher than the other 2 models. and 09 for true negatives in the confusion matrix.


Why Focus on Recall for Class 0 ?

- Accuracy alone might look good (say 79%), but it hides the fact that unhappy customers are being missed.
- By focusing on recall for Class 0, you ensure the model captures as many unhappy customers as possible.
- This directly supports business goals: identifying dissatisfaction → improving operations → reducing churn.

# Feature selection & Hyperparameter Optimizations

Applying a correlation threshold of 0.15 for feature selection means discarding numerical predictors that show only a minimal (absolute correlation <0.15) linear association with the target variable, suggesting limited predictive usefulness. This approach helps lower dimensionality, reduces the risk of overfitting, and enhances overall model effectiveness

Hyperopt is an efficient Python library for hyperparameter optimization that uses a Bayesian optimization approach, It can be used to tune the hyperparameters of various machine learning algorithms, including XGBoost.


# Conclusions :

As demonstrated, the model optimized with Hyperopt significantly outperformed the standard XGB classification model, we identified XGBoost as the most effective model for predicting class 0 (unhappy customers), which is crucial since they represent nearly half of the customer base. After applying advanced hyperparameter optimization, the model achieved an F1 score of 0.80 and an accuracy of 0.77, making it the strongest candidate for this business focus.
